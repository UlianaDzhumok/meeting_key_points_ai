{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is2WNnonbiue"
      },
      "source": [
        "# Подготовка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyGfzJzTbpzy"
      },
      "source": [
        "## Установка зависимостей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KqcvTvZdWygt",
        "outputId": "5a7f3fbc-858a-4768-a9cc-0b28b335cb6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: xformers==0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (0.0.27.post2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.3.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.61.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.5.1+cu124)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.5)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.11/dist-packages (1.4)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.5.1+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.27.post2 peft trl triton==3.1.0\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer openai-whisper soundfile librosa pydantic\n",
        "!pip install --no-deps unsloth ffmpeg\n",
        "!pip install flash-attn --no-build-isolation --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqoDRf6Nb158"
      },
      "source": [
        "## Сервисные функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyX7hjdNWAra"
      },
      "source": [
        "Функция выделения аудио из видео файла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "riMWEFHxWACn"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "def extract_audio(video_path):\n",
        "  \"\"\"\n",
        "    Извлекает аудиодорожку из видеофайла и сохраняет её в формате WAV.\n",
        "\n",
        "    Параметры:\n",
        "    video_path (str): Путь к исходному видеофайлу.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Путь к сохранённому аудиофайлу.\n",
        "\n",
        "    Примечание:\n",
        "    - Используется `ffmpeg` для обработки видеофайла.\n",
        "    - Аудио сохраняется в формате PCM 16 бит с частотой дискретизации 16 кГц.\n",
        "    - Если файл уже существует, он будет перезаписан.\n",
        "    \"\"\"\n",
        "  os.mkdir(\"./outputs\")\n",
        "  audio_path = \"./outputs/extracted_audio.wav\"\n",
        "\n",
        "  subprocess.run([\n",
        "      \"ffmpeg\", \"-i\", video_path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", audio_path, \"-y\"\n",
        "  ])\n",
        "\n",
        "  print(\"Аудио успешно извлечено:\", audio_path)\n",
        "  return audio_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2FFjQXXVbt3"
      },
      "source": [
        "Функция разбиения аудио на части"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nAjnltI2VbKV"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "def split_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Разбивает аудиофайл на фрагменты продолжительностью 30 секунд и сохраняет их в отдельные файлы.\n",
        "\n",
        "    Параметры:\n",
        "    audio_path (str): Путь к исходному аудиофайлу.\n",
        "\n",
        "    Возвращает:\n",
        "    list: Список путей к сохранённым аудиофрагментам.\n",
        "\n",
        "    Примечание:\n",
        "    - Аудиофайл загружается с частотой дискретизации 16 кГц.\n",
        "    - Каждый фрагмент сохраняется в формате WAV.\n",
        "    - Фрагменты сохраняются в папке `./outputs/chunks`.\n",
        "    - Если папка `chunks` не существует, она создаётся автоматически.\n",
        "    \"\"\"\n",
        "    # Загружаем аудио файл\n",
        "    chunks_path=\"./outputs/chunks\"\n",
        "    os.mkdir(chunks_path)\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)  # Проверяем формат 16kHz\n",
        "    chunk_duration = 30  # Выбираем отрезки по 30 секунд\n",
        "    chunk_samples = chunk_duration * sr  # Количество частей\n",
        "\n",
        "    # Разбиваем на части\n",
        "    chunks = [audio[i : i + chunk_samples] for i in range(0, len(audio), chunk_samples)]\n",
        "\n",
        "    # Сохраняем части в отдельные WAV файлы\n",
        "    chunk_paths = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_file = f\"{chunks_path}/audio_chunk_{i}.wav\"\n",
        "        sf.write(chunk_file, chunk, sr)\n",
        "        chunk_paths.append(chunk_file)\n",
        "\n",
        "    print(f\"Аудио разделено на {len(chunk_paths)} частей.\")\n",
        "\n",
        "    return chunk_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbWcNso8Wmnj"
      },
      "source": [
        "Функция транскрибации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GwwY00ZdWqKE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import shutil\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Реализация LoRA (Low-Rank Adaptation) для линейных слоев.\n",
        "\n",
        "    Параметры:\n",
        "    - base_layer (nn.Module): Исходный линейный слой, к которому применяется LoRA.\n",
        "    - r (int): Ранг разложения LoRA (по умолчанию 8).\n",
        "    - alpha (int): Коэффициент масштабирования (по умолчанию 32).\n",
        "    - dropout (float): Вероятность dropout между LoRA-матрицами (по умолчанию 0.2).\n",
        "\n",
        "    Описание:\n",
        "    - Разлагает линейный слой на две дополнительные матрицы (A и B).\n",
        "    - Использует dropout между матрицами для регуляризации.\n",
        "    - После обработки LoRA-слой суммируется с выходом оригинального слоя.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_layer, r=8, alpha=32, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.base_layer = base_layer  # Оригинальный слой\n",
        "        self.r = r  # Ранк\n",
        "        self.scaling = alpha / r\n",
        "\n",
        "        # LoRA A и B матрицы\n",
        "        self.lora_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
        "        self.lora_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
        "\n",
        "         # Dropout между A и B\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Инициализируем LoRA веса\n",
        "        nn.init.kaiming_uniform_(self.lora_A.weight)\n",
        "        nn.init.zeros_(self.lora_B.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_layer(x) + self.lora_B(self.lora_A(x)) * self.scaling\n",
        "\n",
        "def get_transcription(chunk_paths):\n",
        "  \"\"\"\n",
        "  Функция для транскрипции аудиофайлов с использованием модели Whisper и LoRA.\n",
        "\n",
        "  Параметры:\n",
        "  - chunk_paths (list): Список путей к аудиофрагментам.\n",
        "\n",
        "  Возвращает:\n",
        "  - str: Полная транскрипция аудиофайлов.\n",
        "\n",
        "  Описание:\n",
        "  1. Определяет, доступен ли GPU и загружает модель Whisper.\n",
        "  2. Загружает процессор Whisper для обработки аудио.\n",
        "  3. Применяет принудительную настройку языка (русский) для декодера.\n",
        "  4. Заменяет линейные слои модели на LoRA-слои.\n",
        "  5. Загружает предварительно обученные LoRA-веса из Hugging Face.\n",
        "  6. Переводит модель в режим инференса, отключая dropout.\n",
        "  7. Использует `torch.compile()` для ускорения работы модели (если поддерживается).\n",
        "  8. Обрабатывает каждую часть аудио, проводя транскрипцию с Whisper.\n",
        "  9. Собирает транскрибированные части в единый текст.\n",
        "  10. Освобождает память, очищая кэш CUDA и сборщик мусора.\n",
        "  \"\"\"\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # HuggingFace репозиторий для модели\n",
        "  repo_id = \"UDZH/whisper-small-lora-finetuned-ru\"\n",
        "  filename = \"whisper_lora_weights.pth\"\n",
        "  lora_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "\n",
        "  # Загружаем процессор и базовую модель Whisper\n",
        "  model_name=\"openai/whisper-small\"\n",
        "  processor = WhisperProcessor.from_pretrained(model_name)\n",
        "  whisper_model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "  # Принудительное использование русского языка для декодера\n",
        "  whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
        "\n",
        "  # Отключаем dropout перед инференсом\n",
        "  for name, module in whisper_model.named_modules():\n",
        "      if isinstance(module, nn.Linear) and any(k in name for k in [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]):\n",
        "          parent = whisper_model.get_submodule(\".\".join(name.split(\".\")[:-1]))\n",
        "          lora_layer = LoRALayer(module, r=16, alpha=32, dropout=0.4).to(device)\n",
        "          setattr(parent, name.split(\".\")[-1], lora_layer)\n",
        "\n",
        "  # Загружаем веса LoRA и конвертируем их в FP16\n",
        "  lora_weights = torch.load(lora_path, map_location=device)\n",
        "  for k, v in lora_weights.items():\n",
        "      lora_weights[k] = v.half()  # Преобразуем в FP16\n",
        "\n",
        "  # Загружаем LoRA веса в модель\n",
        "  missing_keys, unexpected_keys = whisper_model.load_state_dict(lora_weights, strict=False)\n",
        "  print(\"LoRA веса загружены.\")\n",
        "  print(f\"Пропущенные ключи: {missing_keys}\")\n",
        "  print(f\"Неожиданные ключи: {unexpected_keys}\")\n",
        "\n",
        "  # Переводим в режим инференса и полностью отключаем dropout\n",
        "  whisper_model.eval()\n",
        "  for name, module in whisper_model.named_modules():\n",
        "      if isinstance(module, nn.Dropout):\n",
        "          module.p = 0.0  # Полностью отключаем dropout\n",
        "\n",
        "  # Применяем torch.compile() для ускорения (если поддерживается)\n",
        "  try:\n",
        "      whisper_model = torch.compile(whisper_model)\n",
        "      print(\"torch.compile() успешно применён для ускорения инференса!\")\n",
        "  except AttributeError:\n",
        "      print(\"torch.compile() не поддерживается в данной версии PyTorch, пропускаем...\")\n",
        "\n",
        "  # Проводим транскрипцию для каждой части\n",
        "  transcriptions = []\n",
        "\n",
        "  # Создаем chunk_paths как список путей до частей аудио\n",
        "  for chunk_file in tqdm(chunk_paths, desc=\"Обработка частей\", unit=\"chunk\"):\n",
        "      # Загружаем часть аудио (должно быть 16kHz для Whisper)\n",
        "      audio, _ = librosa.load(chunk_file, sr=16000)\n",
        "\n",
        "      # Токенизируем взодные данные для Whisper\n",
        "      input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
        "\n",
        "      # Транскрибируем\n",
        "      with torch.no_grad():\n",
        "          predicted_ids = whisper_model.generate(input_features)\n",
        "\n",
        "      transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "      transcriptions.append(transcription)\n",
        "\n",
        "  # Склеиваем результаты частей в общую транскрипцию\n",
        "  full_transcription = \" \".join(transcriptions)\n",
        "\n",
        "  # Удаляем все аудио файлы\n",
        "  outputs_path = \"./outputs\"\n",
        "  shutil.rmtree(outputs_path)\n",
        "\n",
        "  # Чистим память и зависшие в памяти тензоры\n",
        "  del whisper_model\n",
        "  del processor\n",
        "  del transcriptions\n",
        "  del transcription\n",
        "  del chunk_file\n",
        "  del audio\n",
        "  del input_features\n",
        "\n",
        "  for obj in gc.get_objects():\n",
        "      try:\n",
        "          if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "              del obj\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.ipc_collect()\n",
        "\n",
        "  return full_transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-f-fyvmYJf"
      },
      "source": [
        "Функции обработки текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTahTviomaxR",
        "outputId": "8ac4f5a8-4e80-482d-85f1-5c47e9245012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def clean_transcript(input_text):\n",
        "    \"\"\"\n",
        "    Функция для очистки транскрипции от шумовых фраз, повторов и лишних символов.\n",
        "\n",
        "    Параметры:\n",
        "    - input_text (str): Исходная транскрипция.\n",
        "\n",
        "    Возвращает:\n",
        "    - str: Очищенный и структурированный текст.\n",
        "\n",
        "    Описание этапов:\n",
        "    1. **Удаление шумовых фраз** – исключает часто встречающиеся бессмысленные фразы и слова-паразиты.\n",
        "    2. **Удаление повторов слов** – убирает последовательные дублирующиеся слова (например, \"финансист финансист\" → \"финансист\").\n",
        "    3. **Сокращение длинных повторяющихся букв** – заменяет длинные последовательности одной буквы (например, \"ээээ\" → \"э\").\n",
        "    4. **Разбиение на предложения** – использует `nltk.sent_tokenize()` для разбиения текста на осмысленные фрагменты.\n",
        "    5. **Фильтрация коротких предложений** – удаляет слишком короткие предложения (менее 3 слов), так как они, скорее всего, являются шумом.\n",
        "    6. **Удаление случайных чисел** – устраняет отдельно стоящие числа, которые могут быть артефактами транскрипции.\n",
        "    7. **Сборка итогового текста** – объединяет отфильтрованные предложения обратно в структурированный текст.\n",
        "    \"\"\"\n",
        "\n",
        "    # Удаляем бессмысленные фразы и шум\n",
        "    common_phrases = [\n",
        "    r\"\\bсбер какие\\b\", r\"\\bджой какие\\b\", r\"\\bафина какие\\b\", r\"\\bтихо\\b\",\n",
        "    r\"\\bкакие деньги\\b\", r\"\\bкакие вопросы\\b\", r\"\\bкакие виды\\b\",\n",
        "    r\"\\bкакие расклады\\b\", r\"\\bкакие правила\\b\", r\"\\bкакие кредиты\\b\",\n",
        "    r\"\\bкакие платежные\\b\", r\"\\bкакие планы\\b\", r\"\\bкакие условия\\b\",\n",
        "    r\"\\bкакие виды у фильма\\b\", r\"\\bкакие виды у города\\b\",\n",
        "    r\"\\bкакие виды у банка\\b\", r\"\\bкакие вопросы мне нужны\\b\",\n",
        "    r\"\\bкакие поечару\\b\", r\"\\bкакие как там\\b\", r\"\\bкакие у города\\b\",\n",
        "    r\"\\bкакие какие\\b\", r\"\\bкакие виды говорить\\b\", r\"\\bкакие что делать\\b\",\n",
        "    r\"\\bпожалуйста\\b\", r\"\\bтихо\\b\", r\"\\bафина\\b\", r\"\\bджой\\b\",\n",
        "    r\"\\bменьше часов по работе\\b\", r\"\\bвопросы у моей семьи\\b\",\n",
        "    r\"\\bбудем оттуда\\b\", r\"\\bбудем менять тогда\\b\", r\"\\bпродолжилась там же\\b\",\n",
        "    r\"\\bсчетом объема\\b\", r\"\\bкуда же вы\\b\", r\"\\bкуда идем\\b\",\n",
        "    r\"\\bгромче если можно\\b\", r\"\\bкак называется\\b\", r\"\\bэто тоже важно\\b\",\n",
        "    r\"\\bработаем не работаем\\b\", r\"\\bэто понятно здесь\\b\", r\"\\bчто нам нужно\\b\",\n",
        "    r\"\\bпосмотреть афина\\b\", r\"\\bпосмотреть как бы\\b\", r\"\\bпосмотреть пробуем\\b\",\n",
        "    r\"\\bкуда перейдем\\b\", r\"\\bну то есть\\b\", r\"\\bто есть\\b\", r\"\\bи уже от этого\\b\",\n",
        "    r\"\\bвиды у фильма\\b\", r\"\\bвиды \\b\",r\"\\bафина\\b\", r\"\\bджой\\b\",r\"\\bсбер\\b\",\n",
        "    r\"\\bсалют среднее расстояние между нептуном и солнцем\\b\", r\"\\bсалют\\b\"]\n",
        "\n",
        "    for phrase in common_phrases:\n",
        "        input_text = re.sub(phrase, \"\", input_text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Убираем повторы слов (например, \"финансист финансист\" → \"финансист\")\n",
        "    input_text = re.sub(r\"\\b(\\w+)\\s+\\1\\b\", r\"\\1\", input_text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Убираем длинные повторяющиеся буквы (например, \"ээээ\" → \"э\")\n",
        "    input_text = re.sub(r\"([a-zA-Zа-яА-Я])\\1{3,}\", r\"\\1\", input_text)\n",
        "\n",
        "    # Разбиваем текст на предложения\n",
        "    sentences = sent_tokenize(input_text)\n",
        "\n",
        "    # Фильтруем слишком короткие и бессмысленные предложения\n",
        "    sentences = [sent for sent in sentences if len(sent.split()) > 3]\n",
        "\n",
        "    # Убираем случайные цифры, стоящие отдельно (например, \"два три 9\" → \"два три\")\n",
        "    sentences = [re.sub(r\"\\b\\d+\\b\", \"\", sent) for sent in sentences]\n",
        "\n",
        "    # Объединяем обратно в текст\n",
        "    cleaned_text = \" \".join(sentences)\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLqRiHO8ZISK"
      },
      "source": [
        "Функция выделения ключевой информации из транскрипции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c9q9twWAZO9l"
      },
      "outputs": [],
      "source": [
        "import unsloth\n",
        "import torch\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Определяем устройство (GPU или CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Определяем структуру JSON-ответа с помощью Pydantic\n",
        "class MeetingSummary(BaseModel):\n",
        "    Summarization: str\n",
        "    Topics: list[str]\n",
        "    Actions: list[str]\n",
        "    Problems: list[str]\n",
        "    Decisions: list[str]\n",
        "\n",
        "# Эталонный JSON-шаблон\n",
        "REFERENCE_JSON = {\n",
        "    \"Summarization\": \"Brief meeting summary...\",\n",
        "    \"Topics\": [\"Topic 1\", \"Topic 2\"],\n",
        "    \"Actions\": [\"Action 1\", \"Action 2\"],\n",
        "    \"Problems\": [\"Problem 1\", \"Problem 2\"],\n",
        "    \"Decisions\": [\"Decision 1\", \"Decision 2\"]\n",
        "    }\n",
        "\n",
        "# Фиксированный промпт без транскрипта\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Analyze the following meeting transcript and extract the key points:\n",
        "1. **Summarization** – a brief summary of the meeting.\n",
        "2. **Topics** – a list of topics discussed.\n",
        "3. **Decisions** – key decisions made.\n",
        "4. **Problems** – challenges or issues identified.\n",
        "5. **Actions** – planned or taken actions.\n",
        "\n",
        "Return the output **STRICTLY in the following JSON format**:\n",
        "{{\n",
        "  \"Summarization\": \"Brief meeting summary...\",\n",
        "  \"Topics\": [\"Topic 1\", \"Topic 2\"],\n",
        "  \"Actions\": [\"Action 1\", \"Action 2\"],\n",
        "  \"Problems\": [\"Problem 1\", \"Problem 2\"],\n",
        "  \"Decisions\": [\"Decision 1\", \"Decision 2\"]\n",
        "}}\n",
        "\n",
        "Meeting transcript (in Russian):\n",
        "{transcript}\n",
        "\n",
        "**Return only a valid JSON response in Russian language.**\n",
        "**Do not include explanations, introductions, or extra text.**\n",
        "**If a category is missing, return an empty array [].**\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def clean_json_fields(data):\n",
        "    \"\"\"\n",
        "    Очищает JSON, исправляя возможные ошибки со списками.\n",
        "    \"\"\"\n",
        "    for key in [\"Topics\", \"Actions\", \"Problems\", \"Decisions\"]:\n",
        "        if isinstance(data.get(key), list):\n",
        "            cleaned_list = []\n",
        "            for item in data[key]:\n",
        "                if isinstance(item, str) and item.startswith(\"[\"):\n",
        "                    try:\n",
        "                        cleaned_list.extend(ast.literal_eval(item))\n",
        "                    except (SyntaxError, ValueError):\n",
        "                        cleaned_list.append(item)\n",
        "                else:\n",
        "                    cleaned_list.append(item)\n",
        "            data[key] = cleaned_list\n",
        "    return data\n",
        "\n",
        "def extract_valid_json(text):\n",
        "    \"\"\"\n",
        "    Извлекает первый JSON, который отличается от шаблонного.\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"^### Ответ:\\s*\", \"\", text)\n",
        "    text = re.sub(r\"^Ответ:\\s*\", \"\", text)\n",
        "\n",
        "    json_matches = re.findall(r\"\\{[\\s\\S]*?\\}\", text)\n",
        "    for json_text in json_matches:\n",
        "        try:\n",
        "            extracted_json = json.loads(json_text)\n",
        "        except json.JSONDecodeError:\n",
        "            try:\n",
        "                extracted_json = ast.literal_eval(json_text)\n",
        "            except (ValueError, SyntaxError):\n",
        "                continue\n",
        "\n",
        "        extracted_json = clean_json_fields(extracted_json)\n",
        "        if extracted_json and extracted_json != REFERENCE_JSON:\n",
        "            return extracted_json\n",
        "    return None\n",
        "\n",
        "def split_transcript(tokenizer, transcript, max_tokens=3000, overlap=300):\n",
        "    \"\"\"\n",
        "    Разбивает стенограмму на части с перекрытием.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(transcript)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = min(start + max_tokens, len(tokens))\n",
        "        chunk = tokenizer.convert_tokens_to_string(tokens[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += max_tokens - overlap\n",
        "    return chunks\n",
        "\n",
        "def generate_response(model, tokenizer, chunk: str) -> str:\n",
        "    \"\"\"\n",
        "    Генерирует ответ модели на основе переданного чанка стенограммы.\n",
        "    \"\"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(transcript=chunk)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=4096).to(device)\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def summarize_chunk(model, tokenizer, chunk):\n",
        "    \"\"\"\n",
        "    Обрабатывает отдельный чанк стенограммы, создавая JSON-резюме.\n",
        "\n",
        "    Параметры:\n",
        "    - model: Модель для обработки текста.\n",
        "    - tokenizer: Токенизатор модели.\n",
        "    - chunk (str): Часть стенограммы.\n",
        "\n",
        "    Возвращает:\n",
        "    - dict: JSON-объект с ключевыми моментами.\n",
        "    \"\"\"\n",
        "    response = generate_response(model, tokenizer, chunk)\n",
        "    json_data = extract_valid_json(response)\n",
        "    return json_data if json_data else {}\n",
        "\n",
        "def merge_chunks(chunks):\n",
        "    \"\"\"\n",
        "    Объединяет JSON-резюме всех чанков в один итоговый JSON.\n",
        "\n",
        "    Параметры:\n",
        "    - chunks (list): Список JSON-объектов с ключевыми моментами.\n",
        "\n",
        "    Возвращает:\n",
        "    - dict: Итоговое объединённое резюме.\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return {\n",
        "            \"Summarization\": \"\",\n",
        "            \"Topics\": [],\n",
        "            \"Actions\": [],\n",
        "            \"Problems\": [],\n",
        "            \"Decisions\": []\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"Summarization\": \" \".join(chunk.get(\"Summarization\", \"\") for chunk in chunks),\n",
        "        \"Topics\": list(set(topic for chunk in chunks for topic in chunk.get(\"Topics\", []))),\n",
        "        \"Actions\": list(set(action for chunk in chunks for action in chunk.get(\"Actions\", []))),\n",
        "        \"Problems\": list(set(problem for chunk in chunks for problem in chunk.get(\"Problems\", []))),\n",
        "        \"Decisions\": list(set(decision for chunk in chunks for decision in chunk.get(\"Decisions\", []))),\n",
        "    }\n",
        "\n",
        "def extract_meeting_summary(model, tokenizer, input_text: str, retry_attempts=3):\n",
        "    \"\"\"\n",
        "    Обрабатывает стенограмму и извлекает ключевые моменты встречи.\n",
        "\n",
        "    Параметры:\n",
        "    - model: Модель для обработки текста.\n",
        "    - tokenizer: Токенизатор модели.\n",
        "    - input_text (str): Входной текст стенограммы.\n",
        "    - retry_attempts (int): Количество повторных попыток в случае неудачи.\n",
        "\n",
        "    Возвращает:\n",
        "    - dict: Итоговое резюме встречи.\n",
        "    \"\"\"\n",
        "    for _ in range(retry_attempts):\n",
        "        chunks = split_transcript(tokenizer, input_text)\n",
        "        chunk_summaries = [summarize_chunk(model, tokenizer, chunk) for chunk in chunks]\n",
        "\n",
        "        if chunk_summaries:\n",
        "            return merge_chunks(chunk_summaries)\n",
        "\n",
        "    return {\n",
        "        \"Summarization\": \"\",\n",
        "        \"Topics\": [],\n",
        "        \"Actions\": [],\n",
        "        \"Problems\": [],\n",
        "        \"Decisions\": []\n",
        "    }\n",
        "\n",
        "def get_meeting_info(input_text):\n",
        "    \"\"\"\n",
        "    Основная функция для обработки стенограммы встречи.\n",
        "\n",
        "    Параметры:\n",
        "    - input_text (str): Входной текст стенограммы.\n",
        "\n",
        "    Возвращает:\n",
        "    - dict: Итоговое резюме встречи.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = \"UDZH/deepseek-meeting-summary\"\n",
        "\n",
        "    deepseek_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name, max_seq_length=16_384, dtype=None, load_in_4bit=True, device_map='auto',\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    deepseek_model = FastLanguageModel.for_inference(deepseek_model)\n",
        "    result = extract_meeting_summary(deepseek_model, tokenizer, input_text)\n",
        "\n",
        "    del deepseek_model, tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZm9iXqyIll"
      },
      "source": [
        "Форматирование вывода для чтения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SJnbjc1cyGUl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def format_transcription(text):\n",
        "  \"\"\"\n",
        "  Форматирует текст стенограммы, разбивая его на отдельные предложения.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): Исходный текст транскрипции.\n",
        "\n",
        "  Возвращает:\n",
        "  - str: Отформатированный текст, где каждое предложение начинается с новой строки.\n",
        "\n",
        "  Описание:\n",
        "  1. Разбивает текст на предложения по знакам окончания (. ! ?), при этом знаки остаются в тексте.\n",
        "  2. Использует `re.split()` с регулярным выражением для корректного разбиения.\n",
        "  3. Удаляет лишние пробелы в начале и конце текста.\n",
        "  4. Объединяет предложения, размещая каждое на новой строке.\n",
        "  \"\"\"\n",
        "\n",
        "  # Разбиваем текст по знакам окончания предложений (. ! ?), оставляя их в тексте\n",
        "  sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "  # Выводим каждое предложение на новой строке\n",
        "  formatted_text = \"\\n\".join(sentences)\n",
        "  return formatted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eGh2rpb7yx83"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def clean_list(lst):\n",
        "    \"\"\"\n",
        "    Очищает список от вложенных строковых списков, убирает дубликаты и лишние символы.\n",
        "\n",
        "    Параметры:\n",
        "    - lst (list): Исходный список строк.\n",
        "\n",
        "    Возвращает:\n",
        "    - list: Очищенный список без дубликатов и лишних символов.\n",
        "\n",
        "    Описание:\n",
        "    1. Убирает кавычки («»), пробелы в начале и конце строк.\n",
        "    2. Проверяет, содержится ли элемент в формате строкового списка (например, '[\"a\", \"b\"]') и преобразует его в список.\n",
        "    3. Разбивает элементы, содержащие запятые, и добавляет их в список отдельно.\n",
        "    4. Удаляет дубликаты, сохраняя только уникальные элементы.\n",
        "    \"\"\"\n",
        "\n",
        "    seen = set()\n",
        "    cleaned_list = []\n",
        "\n",
        "    for item in lst:\n",
        "        if isinstance(item, str):\n",
        "            item = item.strip().replace(\"«\", \"\").replace(\"»\", \"\")\n",
        "\n",
        "            # Проверяем, является ли item строковым списком (например, '[\"a\", \"b\"]')\n",
        "            if item.startswith(\"[\") and item.endswith(\"]\"):\n",
        "                try:\n",
        "                    parsed_item = ast.literal_eval(item)  # Преобразуем строку в список\n",
        "                    if isinstance(parsed_item, list):\n",
        "                        for sub_item in parsed_item:\n",
        "                            sub_item = str(sub_item).strip().replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "                            if sub_item and sub_item not in seen:\n",
        "                                cleaned_list.append(sub_item)\n",
        "                                seen.add(sub_item)\n",
        "                        continue  # Пропускаем добавление исходной строки\n",
        "                except (ValueError, SyntaxError):\n",
        "                    pass  # Если ошибка, рассматриваем как обычную строку\n",
        "\n",
        "        # Разбиваем по запятым, если в строке несколько пунктов\n",
        "        if \",\" in item:\n",
        "            sub_items = [x.strip() for x in item.split(\",\") if x.strip()]\n",
        "            for sub_item in sub_items:\n",
        "                if sub_item and sub_item not in seen:\n",
        "                    cleaned_list.append(sub_item)\n",
        "                    seen.add(sub_item)\n",
        "            continue  # Не добавляем исходную строку повторно\n",
        "\n",
        "        if item and item not in seen:\n",
        "            cleaned_list.append(item)\n",
        "            seen.add(item)\n",
        "\n",
        "    return cleaned_list\n",
        "\n",
        "def remove_duplicates(lst):\n",
        "    \"\"\"\n",
        "    Удаляет дубликаты из списка, сохраняя порядок элементов.\n",
        "\n",
        "    Параметры:\n",
        "    - lst (list): Исходный список строк.\n",
        "\n",
        "    Возвращает:\n",
        "    - list: Список без дубликатов, с сохранением исходного порядка.\n",
        "\n",
        "    Описание:\n",
        "    1. Приводит строки к нижнему регистру и убирает пробелы перед добавлением в `set`, чтобы избежать повторений.\n",
        "    2. Сохраняет формат исходных строк (например, регистр и пробелы) при возврате результата.\n",
        "    \"\"\"\n",
        "\n",
        "    seen = set()\n",
        "    cleaned = []\n",
        "    for x in lst:\n",
        "        cleaned_x = x.strip().lower()  # Приводим к нижнему регистру, убираем пробелы\n",
        "        if cleaned_x not in seen:\n",
        "            seen.add(cleaned_x)\n",
        "            cleaned.append(x.strip())  # Сохраняем оригинальный формат строки\n",
        "    return cleaned\n",
        "\n",
        "def clean_meeting_summary(meeting_info):\n",
        "    \"\"\"\n",
        "    Очищает JSON-данные о встрече от дубликатов и вложенных списков.\n",
        "\n",
        "    Параметры:\n",
        "    - meeting_info (dict): JSON-объект с данными о встрече.\n",
        "\n",
        "    Возвращает:\n",
        "    - dict: Очищенные JSON-данные.\n",
        "\n",
        "    Описание:\n",
        "    1. Очищает поля `Topics`, `Actions`, `Problems` и `Decisions`, убирая дубликаты и вложенные строки-списки.\n",
        "    2. Разбивает резюме встречи (`Summarization`) на предложения и убирает дубликаты.\n",
        "    3. Собирает обновленные данные обратно в JSON-объект.\n",
        "    \"\"\"\n",
        "\n",
        "    # Очистка списков + удаление дубликатов\n",
        "    for key in [\"Topics\", \"Actions\", \"Problems\", \"Decisions\"]:\n",
        "        meeting_info[key] = remove_duplicates(clean_list(meeting_info.get(key, [])))\n",
        "\n",
        "    # Очистка резюме (по предложениям) с учётом пробелов и регистра\n",
        "    if \"Summarization\" in meeting_info:\n",
        "        sentences = [s.strip() for s in meeting_info[\"Summarization\"].split('.') if s.strip()]\n",
        "        meeting_info[\"Summarization\"] = '. '.join(remove_duplicates(sentences))\n",
        "\n",
        "    return meeting_info\n",
        "\n",
        "def format_meeting_info(meeting_info):\n",
        "    \"\"\"\n",
        "    Форматирует JSON-данные о встрече в удобочитаемый текст.\n",
        "\n",
        "    Параметры:\n",
        "    - meeting_info (dict): JSON-объект с данными о встрече.\n",
        "\n",
        "    Возвращает:\n",
        "    - str: Отформатированный текст с разделами по ключевым аспектам встречи.\n",
        "\n",
        "    Описание:\n",
        "    1. Разбивает резюме (`Summarization`) на строки для лучшей читаемости.\n",
        "    2. Формирует структурированный текст с заголовками:\n",
        "      - **Резюме встречи**\n",
        "      - **Темы обсуждения**\n",
        "      - **Действия**\n",
        "      - **Проблемы**\n",
        "      - **Решения**\n",
        "    3. Если раздел пуст, добавляет соответствующую подпись (\"Нет тем.\", \"Нет действий.\" и т. д.).\n",
        "    \"\"\"\n",
        "\n",
        "    summary_text = '\\n'.join(meeting_info.get('Summarization', '').split('. '))\n",
        "    formatted_text = f\"**Резюме встречи:**\\n{summary_text}\\n\\n\"\n",
        "\n",
        "    formatted_text += \"**Темы обсуждения:**\\n\"\n",
        "    topics = meeting_info.get('Topics', [])\n",
        "    formatted_text += \"\\n\".join(f\"- {topic}\" for topic in topics) + \"\\n\\n\" if topics else \"Нет тем.\\n\\n\"\n",
        "\n",
        "    formatted_text += \"**Действия:**\\n\"\n",
        "    actions = meeting_info.get('Actions', [])\n",
        "    formatted_text += \"\\n\".join(f\"- {action}\" for action in actions) + \"\\n\\n\" if actions else \"Нет действий.\\n\\n\"\n",
        "\n",
        "    formatted_text += \"**Проблемы:**\\n\"\n",
        "    problems = meeting_info.get('Problems', [])\n",
        "    formatted_text += \"\\n\".join(f\"- {problem}\" for problem in problems) + \"\\n\\n\" if problems else \"Нет проблем.\\n\\n\"\n",
        "\n",
        "    formatted_text += \"**Решения:**\\n\"\n",
        "    decisions = meeting_info.get('Decisions', [])\n",
        "    formatted_text += \"\\n\".join(f\"- {decision}\" for decision in decisions) + \"\\n\" if decisions else \"Нет решений.\\n\"\n",
        "\n",
        "    return formatted_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jJ8dCrFU4Cb"
      },
      "source": [
        "# Получение транскрипции из видео и определение ключевой информации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJe1xob5UlsA",
        "outputId": "7652f1cb-d735-4f0c-fed3-9c7a9b3c4532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Аудио успешно извлечено: ./outputs/extracted_audio.wav\n",
            "Аудио разделено на 97 частей.\n",
            "LoRA веса загружены.\n",
            "Пропущенные ключи: ['model.encoder.conv1.weight', 'model.encoder.conv1.bias', 'model.encoder.conv2.weight', 'model.encoder.conv2.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.0.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.0.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.0.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.0.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.base_layer.weight', 'model.encoder.layers.0.fc1.base_layer.bias', 'model.encoder.layers.0.fc2.base_layer.weight', 'model.encoder.layers.0.fc2.base_layer.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.1.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.1.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.1.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.1.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.1.fc1.base_layer.weight', 'model.encoder.layers.1.fc1.base_layer.bias', 'model.encoder.layers.1.fc2.base_layer.weight', 'model.encoder.layers.1.fc2.base_layer.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.2.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.2.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.2.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.2.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc1.base_layer.weight', 'model.encoder.layers.2.fc1.base_layer.bias', 'model.encoder.layers.2.fc2.base_layer.weight', 'model.encoder.layers.2.fc2.base_layer.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.3.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.3.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.3.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.3.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc1.base_layer.weight', 'model.encoder.layers.3.fc1.base_layer.bias', 'model.encoder.layers.3.fc2.base_layer.weight', 'model.encoder.layers.3.fc2.base_layer.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.4.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.4.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.4.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.4.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.4.fc1.base_layer.weight', 'model.encoder.layers.4.fc1.base_layer.bias', 'model.encoder.layers.4.fc2.base_layer.weight', 'model.encoder.layers.4.fc2.base_layer.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.5.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.5.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.5.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.5.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.5.fc1.base_layer.weight', 'model.encoder.layers.5.fc1.base_layer.bias', 'model.encoder.layers.5.fc2.base_layer.weight', 'model.encoder.layers.5.fc2.base_layer.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.6.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.6.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.6.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.6.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.base_layer.weight', 'model.encoder.layers.6.fc1.base_layer.bias', 'model.encoder.layers.6.fc2.base_layer.weight', 'model.encoder.layers.6.fc2.base_layer.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.7.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.7.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.7.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.7.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.base_layer.weight', 'model.encoder.layers.7.fc1.base_layer.bias', 'model.encoder.layers.7.fc2.base_layer.weight', 'model.encoder.layers.7.fc2.base_layer.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.8.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.8.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.8.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.8.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.8.fc1.base_layer.weight', 'model.encoder.layers.8.fc1.base_layer.bias', 'model.encoder.layers.8.fc2.base_layer.weight', 'model.encoder.layers.8.fc2.base_layer.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.9.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.9.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.9.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.9.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.9.fc1.base_layer.weight', 'model.encoder.layers.9.fc1.base_layer.bias', 'model.encoder.layers.9.fc2.base_layer.weight', 'model.encoder.layers.9.fc2.base_layer.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.10.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.10.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.10.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.10.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc1.base_layer.weight', 'model.encoder.layers.10.fc1.base_layer.bias', 'model.encoder.layers.10.fc2.base_layer.weight', 'model.encoder.layers.10.fc2.base_layer.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'model.encoder.layers.11.self_attn.v_proj.base_layer.weight', 'model.encoder.layers.11.self_attn.v_proj.base_layer.bias', 'model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.11.self_attn.out_proj.base_layer.weight', 'model.encoder.layers.11.self_attn.out_proj.base_layer.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc1.base_layer.weight', 'model.encoder.layers.11.fc1.base_layer.bias', 'model.encoder.layers.11.fc2.base_layer.weight', 'model.encoder.layers.11.fc2.base_layer.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layer_norm.weight', 'model.encoder.layer_norm.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.0.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.0.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.0.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.0.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.0.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.0.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.0.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.0.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.0.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.0.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.0.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.0.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.base_layer.weight', 'model.decoder.layers.0.fc1.base_layer.bias', 'model.decoder.layers.0.fc2.base_layer.weight', 'model.decoder.layers.0.fc2.base_layer.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.1.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.1.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.1.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.1.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.1.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.1.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.1.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.1.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.1.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.1.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.1.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.1.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.base_layer.weight', 'model.decoder.layers.1.fc1.base_layer.bias', 'model.decoder.layers.1.fc2.base_layer.weight', 'model.decoder.layers.1.fc2.base_layer.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.2.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.2.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.2.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.2.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.2.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.2.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.2.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.2.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.2.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.2.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.2.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.2.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.base_layer.weight', 'model.decoder.layers.2.fc1.base_layer.bias', 'model.decoder.layers.2.fc2.base_layer.weight', 'model.decoder.layers.2.fc2.base_layer.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.3.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.3.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.3.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.3.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.3.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.3.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.3.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.3.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.3.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.3.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.3.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.3.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.base_layer.weight', 'model.decoder.layers.3.fc1.base_layer.bias', 'model.decoder.layers.3.fc2.base_layer.weight', 'model.decoder.layers.3.fc2.base_layer.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.4.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.4.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.4.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.4.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.4.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.4.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.4.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.4.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.4.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.4.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.4.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.4.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.base_layer.weight', 'model.decoder.layers.4.fc1.base_layer.bias', 'model.decoder.layers.4.fc2.base_layer.weight', 'model.decoder.layers.4.fc2.base_layer.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.5.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.5.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.5.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.5.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.5.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.5.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.5.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.5.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.5.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.5.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.5.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.5.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.base_layer.weight', 'model.decoder.layers.5.fc1.base_layer.bias', 'model.decoder.layers.5.fc2.base_layer.weight', 'model.decoder.layers.5.fc2.base_layer.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.6.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.6.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.6.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.6.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.6.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.6.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.6.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.6.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.6.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.6.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.6.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.6.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.base_layer.weight', 'model.decoder.layers.6.fc1.base_layer.bias', 'model.decoder.layers.6.fc2.base_layer.weight', 'model.decoder.layers.6.fc2.base_layer.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.7.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.7.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.7.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.7.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.7.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.7.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.7.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.7.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.7.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.7.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.7.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.7.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.base_layer.weight', 'model.decoder.layers.7.fc1.base_layer.bias', 'model.decoder.layers.7.fc2.base_layer.weight', 'model.decoder.layers.7.fc2.base_layer.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.8.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.8.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.8.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.8.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.8.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.8.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.8.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.8.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.8.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.8.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.8.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.8.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.8.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.base_layer.weight', 'model.decoder.layers.8.fc1.base_layer.bias', 'model.decoder.layers.8.fc2.base_layer.weight', 'model.decoder.layers.8.fc2.base_layer.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.9.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.9.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.9.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.9.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.9.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.9.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.9.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.9.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.9.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.9.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.9.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.9.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.weight', 'model.decoder.layers.9.encoder_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.base_layer.weight', 'model.decoder.layers.9.fc1.base_layer.bias', 'model.decoder.layers.9.fc2.base_layer.weight', 'model.decoder.layers.9.fc2.base_layer.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.10.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.10.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.10.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.10.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.10.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.10.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.10.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.10.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.10.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.10.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.10.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.10.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.10.encoder_attn_layer_norm.weight', 'model.decoder.layers.10.encoder_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.base_layer.weight', 'model.decoder.layers.10.fc1.base_layer.bias', 'model.decoder.layers.10.fc2.base_layer.weight', 'model.decoder.layers.10.fc2.base_layer.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.base_layer.weight', 'model.decoder.layers.11.self_attn.v_proj.base_layer.weight', 'model.decoder.layers.11.self_attn.v_proj.base_layer.bias', 'model.decoder.layers.11.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.11.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.11.self_attn.out_proj.base_layer.weight', 'model.decoder.layers.11.self_attn.out_proj.base_layer.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.encoder_attn.k_proj.base_layer.weight', 'model.decoder.layers.11.encoder_attn.v_proj.base_layer.weight', 'model.decoder.layers.11.encoder_attn.v_proj.base_layer.bias', 'model.decoder.layers.11.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.11.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.11.encoder_attn.out_proj.base_layer.weight', 'model.decoder.layers.11.encoder_attn.out_proj.base_layer.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.weight', 'model.decoder.layers.11.encoder_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.base_layer.weight', 'model.decoder.layers.11.fc1.base_layer.bias', 'model.decoder.layers.11.fc2.base_layer.weight', 'model.decoder.layers.11.fc2.base_layer.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layer_norm.bias', 'proj_out.weight']\n",
            "Неожиданные ключи: []\n",
            "torch.compile() успешно применён для ускорения инференса!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Обработка частей: 100%|██████████| 97/97 [03:15<00:00,  2.02s/chunk]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.3.5: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "Транскрипция встречи\n",
            "-----------------------------------------------------------------\n",
            "you  you  you  Yn ymwneud yma, yw'r cyffredin yn ymwneud yma.\n",
            "У меня слышно, Наталья?\n",
            "Да, слышно.\n",
            "Говорить сейчас есть возможность?\n",
            "Мы говорили там...\n",
            "Да, они с собой занимаются.\n",
            "Поэтому есть спокойного там планения, неопременница здесь поэтому никто не нужен.\n",
            "Ну понятно.\n",
            "Так, чего у нас по пачке вопросы?\n",
            "Мы закончили, человек подходит.\n",
            "Я не слышал по последнему.\n",
            "Мы обсуждали.\n",
            "Мы обсуждали.\n",
            "Мы говорили, что руководитель склада.\n",
            "Мы закончили, что посмотрим, поменяем портрет.\n",
            "Попробуем.\n",
            "Пока все не сменяем.\n",
            "Не снимаем.\n",
            "Ищем старшего.\n",
            "Закод будет через соста.\n",
            "Руководитель склада.\n",
            "И до поездки.\n",
            "Дальше посмотрим, если нужна будет смена.\n",
            "Финансист в процессе не срочно.\n",
            "Я не вчера обозначила приоритет и бухгалтер старший водовщик, руководитель склада.\n",
            "Дальше по рейтингу девочку сильнее нашли.\n",
            "С 9 числа попробуем ее вывести.\n",
            "В группе тихо я посмотрела.\n",
            "Пробуем с ней.\n",
            "Двух и чаров выводим тоже с 9-10 числа.\n",
            "На стажировку и попробуем быстро посмотреть их.\n",
            "Время не полить.\n",
            "Финансисты ищем в процессе.\n",
            "Дальше менеджер ВБ.\n",
            "Уфлайн.\n",
            "Я так понимаю.\n",
            "Если два рабочих места нужно.\n",
            "Есть смысл посмотреть кандидатов, которые в офлайне будут.\n",
            "Вопрос, конечно, мы сможем им в офис найти.\n",
            "Но там люди именно в те условия, которые есть.\n",
            "Но начинать мы в любом случае  и  и по Valberist.\n",
            "и обновление код обновления кабинета у Дмитрия 9 числа произойдет пока я не вижу смысла наверное покупать контакты и размещение вакальтип потому что в любом случае тихо ни вакальтип не соискать или там минимум сам то есть мы в зря в зря потратим все равно ничего не найдем вот поэтому да зон начинаем и то еще помощник операционного  управляющего я не обозначила что скажу после праздника что открываем посмотрю вот эту анастасию на админа мы ее оставим может ее поддаучивать надо времени у нас не будет либо посмотрю сколько она возьмет второй вариант да из того что рассматривая если возьмет больше обсуждали потому что раз до восьми часов какие-то операционные задачи то может быть  Можем вам, Женя, привет.\n",
            "Жду вас в гости.\n",
            "Это то что по ЮЧАРу в серверо обсудили.\n",
            "По закупке обсудили Юлия сказала что берем 2000, что от выставит.\n",
            "И написала что минимум еще 1000 нам намного намного.\n",
            "Соворилось какой то из пабли, чтобы нам намного добрать объем.\n",
            "Вот нам бы да еще минимум 1000 я посмотрела по таблице.\n",
            "Возьмите, что мы сейчас откроем.\n",
            "и  тогда сделан.\n",
            "Дальше заключение договора эксклюзива по России.\n",
            "Давайте после 10-го.\n",
            "Сейчас они работают.\n",
            "Тут надо по времени смотреть, что у нас там получается.\n",
            "Если время есть, то можно сейчас.\n",
            "Если время нет, то можно чуть попозже.\n",
            "Потому что основная деятельность начнется как раз после 10-го, но к ним начнут сыпаться.\n",
            "Хотя, может быть, народ живет не по нашим праздникам.\n",
            "То есть это у нас в России, в России до 8-го.\n",
            "Во всех остальных странах 1-2.\n",
            "Они, я думаю, в обычном режиме уже работают.\n",
            "Смотрите, по помощнику, там просто светлана закрывала часть задач, а по взаимодействию, насколько я понимаю, сэр с ними, точнее сэр с ними, а с этими, с компаниями дизайна и по сертификатам еще почему-то.\n",
            "То есть, вот нам не потерять эту работу, да, и кому-то передать ее,  она продолжилась в том же самом крюче.\n",
            "не контролировался.\n",
            "То есть, наоборот, был под контролем у кого-то, да, и включаться туда во все эти промежуточные задачи.\n",
            "Она должна прописать процесс за автору встречи, посмотрим, где она участвует и что.\n",
            "И уже от этого будем исходить.\n",
            "По выкупам Сергею говорила, что цвета уходит, нужно передавать.\n",
            "Передавать будем в...\n",
            "Ну там, я настолько вижу, там передавать особо некому.\n",
            "Максим просаживается, не выполняет задачу вовремя.\n",
            "Смотрим по собеседру.\n",
            "либо да у него сотрудники на отзывах им не вытянуть да они справляются они получается тогда смысл наполовину держать непонятно пока вот попробую найти сильного и там дальше решим дальше это по отзывам по выкупам посмотрим просто так ну не передадим если справляться будут и передадим  пока свет у нас в январе есть будем постепенно попробовать ее и вводить и пропишем процесс заберем по тому, чтобы не потерять и у нас стопро не встало где-то потому что здесь наконец-то отладили вроде бы все пошло так как надо ну да, процесс нормально выстроился я тоже видел  и оплату все они не делают, а все остальное вроде бы у нас запустилось и пошло.\n",
            "Так, что у нас еще загрузка региональных сложов?\n",
            "Ну, видимо нет.\n",
            "Ну, ей нужно остаться.\n",
            "Пока нет, но она проговорила, наверное, когда мы пятницу или субботу общались в последний раз, она проговорила, что если бы у нее было  меньше часов по работе, но она бы всё успевала делать и может быть и останется посмотрим.\n",
            "Посмотрит как бы ну политика интересная, это не первый раз да например и свет и просто близко, то есть мы работаем в 6 часов рабочий день, ничего не успеваем, задачи не закрываем, но если я закрощу день до 4 часов  Я буду успевать больше и все будет хорошо.\n",
            "Самое главное, что так и происходит.\n",
            "Если сократим задачи, будут в том же объеме выполняться, но будет лучше.\n",
            "Зарплата меньше, чисто меньше, но задачи будут выполняться.\n",
            "может быть и дизайнеры дать, и эти два процесса обозначат какие-то время или суммы, и какой-то либо переходный период, то есть посмотреть, пока мы ищем людей, либо может быть так и оставить.\n",
            "Я об этом тоже уже думала, что вот эти постоянные процессы ей вроде бы нравятся, вроде бы она делает, и они построились под нее, вместе с ней выстраивались и пошли, может быть да,  пока замену не найдем, хорошо, как и оставить, но видно будет.\n",
            "Я не переправлял вам.\n",
            "не об этом нет там другой видео он еще отправил ну суть там видео в принципе как бы показывать с какого склада идет а я хотел просто встречу провести и проговорить о том что нам нужно планировать загрузки складов то есть сейчас у нас получается есть ипешки да есть разные марки плейс если мы берем в развесе валдереса вот там на 4 ипешки у нас там око сейчас не работает мы прогружаем  то есть товар.\n",
            "Сейчас мы специализируемся больше на том, что тот товар, который мы продаем, чтобы оно было в достатке.\n",
            "И его, соответственно, размножаем на разные ипешки.\n",
            "Дальше иду уже после того, как мы размножены на ипешки.\n",
            "Результат работы карточки зависит от загруженных деткладов.\n",
            "То есть вот эта карточка делится на количество складов, на которых там присутствует этот товар.\n",
            "И вот я бы посмотрел, то бы прогнозировать и размещать эти склады.\n",
            "Потому что мне как  что где-то мы по каким-то складам ведем просадку.\n",
            "Я это информацию не вижу, там обладает ей Сергей, он там принимает решение.\n",
            "Я бы ее подсвечивал, то есть смотрел бы, потому что у нас некоторые успехи и просадки могут быть связаны как раз таки с этим.\n",
            "Он не может побороться там отправить какой-то товар на склад, потому что нет места машины, там еще что-то.\n",
            "И мы там в этом просадываемся.\n",
            "Обанально там просто нужно привести людей, которые найдут машину, там отправят и  Мы загрузим тот необходимый склад.\n",
            "И плюс экономика еще тут важна.\n",
            "Как раз экономика показывается в этом видео от Дениса.\n",
            "Их показывает, что из этого склада человек заказывает в Екатеринбург.\n",
            "Например, логистика для нас такая.\n",
            "А если мы в Екатеринбург отправили, то логистик снижается, он у нас арендабельный.\n",
            "Вот такой вот работой не хватает.\n",
            "что я об этом сходится в одно место.\n",
            "На собеседование кто-то из финансистов девочка, по-моему, должна вывести яна на тест.\n",
            "Она говорила о том, что...\n",
            "Я Сергею об этом потом писала, но она говорила о том, что они просчитывают логистику складов, то есть они продают в компании, в котором она работает, то есть они продают, они загружают на склады  и у них была арентабельность товара одна.\n",
            "В какой-то момент арентабельность резко упала, расходы резко выросли.\n",
            "Они стали смотреть и выяснили, что как раз-таки вот да, наверное, то, о чем вы говорите, она немножко подбегомо это подносила.\n",
            "То есть то, что получается от грузка идет, к примеру, с коледина, в покупателю в Екатеринбурге.\n",
            "И за счет этого, за счет количества промежуточных складов, у них выросла логистика и арентабельность вниз пошла.\n",
            "и они тогда меняли склады, какие-то склады убирали, какие-то добавляли и они меняли товары, потому что логистика на них тоже, ну вот за счет этого перемещения по складам она тоже увеличивалась, росла.\n",
            "Здесь, наверное, как раз тоже об этом получается.\n",
            "Ну я, в принципе, говорю о том, чтобы разместиться на всех складах, а здесь, ну как раз-таки, идет выводы, да, уже, ну то есть разместились на всех складах, ну там или там на каких-то, казалось,  Это тоже важная составляющая, то есть, прежде чем размещаться, просчитать.\n",
            "То есть, тут работа логиста и финансиста, грубо говоря, чтобы можно было задействовать.\n",
            "Потому что одному, наверное, тут тяжело будет сложно.\n",
            "Вот в эту сторону, наверное, будет какое-то определенное развитие еще у нас.\n",
            "Потому что мы еще начинаем специализироваться на одном и том же товаре.\n",
            "То есть, товары много, его нужно продавать.\n",
            "Как продавать?\n",
            "Его нужно же вмещать на EBA, как карчик создавать.\n",
            "Его нужно же вмещать на разный складах.\n",
            "тогда будет эффективность.\n",
            "просто проговорим на встречку, когда будем.\n",
            "Это просто один из вопросов.\n",
            "Я его просто вот так вижу.\n",
            "Но я его не просчитывал, не смотрел.\n",
            "То есть нужно обратная связь.\n",
            "Как они составляют, как они принимают решение, как они делают, отправлять, не отправлять, как они смотрят.\n",
            "Потому что может выплывать несколько каких-то вещей, которыми мы, может, не знаем.\n",
            "Хорошо.\n",
            "Прописала себе и нам нужно решить.\n",
            "Вместе с этим мы будем делать все, что мы делаем.\n",
            "накопившиеся, которые у нас есть сейчас задачи подокрыть в вакансе, поведение должность и потом уже планово туда переступить.\n",
            "в  и  Доберем первую очередь и вторую задачу нужно с Люда начала, но никуда не сдвинулась, дальше не ушла.\n",
            "Нужно перевести на Google аккаунты рабочие всех сотрудников по отделам, то что мы говорили, у нас тогда привязка будет к нашим аккаунтам.\n",
            "И доступа нам не нужно будет открывать и контролировать.\n",
            "И удаленные столы нужно всем привязать, потому что то, о чем говорили на встрече с Романом,  У нас много, много есть рабочих столов, ну, вдалённые вот эти тостки по пользователю.\n",
            "Они далеко не все работают.\n",
            "И ребята там, к примеру, менеджер, они работают только в Одиннадске.\n",
            "Там нет какой-то мега работы.\n",
            "По браузерам знаю, почему делали.\n",
            "Здесь вопрос может быть нам пока...\n",
            "Ну, с одной стороны, да, то есть браузеры мы заврозили для того, чтобы открывать кабинеты Marketplace.\n",
            "С на удаленных столах сотрудникам и работой.\n",
            "В одном браузере не открыть все четыре кабинета в аутболе.\n",
            "Просто по основным кабинетам я даю просто с компа.\n",
            "То есть я на удаленный не захожу.\n",
            "Люда мне кажется тоже.\n",
            "Может быть есть смысл пока мы здесь с серверами ищем убрать эти лифтные браузеры, чтобы сервера стала легче.\n",
            "Но с другой стороны это дополнительно  Работа и часы Ruxes, которые нужно будет оплачивать.\n",
            "А смысл?\n",
            "Нет, я думаю, что до конца инваря мы куда-то переедем уже.\n",
            "Поэтому, да, наверное, да.\n",
            "Мог и по браузерам решим.\n",
            "На новый сервер может быть пока и не нужно будет столько браузера, по основной кабинеты просто останутся.\n",
            "Работа с приказами.\n",
            "задача стоит.\n",
            "Да, нам нужно ее построить.\n",
            "У нас сейчас какая система идет, почему я не обозначал.\n",
            "Приказ мы выпустили по поводу новогодних всех встальных историй и праздников.\n",
            "У Дмитрия какое-то свое расписание оказалось, которое у меня праздник.\n",
            "Вот какие-то банальные вопросы, на которые у нас должны системно быть ответы.\n",
            "Они просто внутреннее раздражение вызывают, потому что есть приказ, мы работаем в этом ключе.\n",
            "А у меня вот тогда-то, тогда-то.\n",
            "То есть какое-то недопонимание это односною одного человека в целом.\n",
            "И как бы по приказам нам нужно историю, наверное, запускать и следовать.\n",
            "То есть я могу где-то в каких-то моментах сам просаживаться, то есть просто уходить либо вовремя там не подписывать.\n",
            "А здесь нужно построить какую-то систему, чтобы она все-таки у нас работала.\n",
            "И мы на нее ссылались и работали только по ней.\n",
            "Чтобы не было.\n",
            "То есть как-то в этом ключе проработать.\n",
            "Но здесь можно либо переходить обратно.\n",
            "Лена Годецкая пыталась вести, но мы так-то не перешли.\n",
            "Там был ослаблён отчет от приказов 5.\n",
            "вот с приказами не ушли.\n",
            "Просто вот насколько нужно, нам в них упадить не будет эта дуракратия.\n",
            "Может быть.\n",
            "Есть тоже какие-то опасения по этому поводу.\n",
            "Я бы здесь, ну, какой-то опыт взял бы, ну, то есть какой-то это и сам, потому что приказы это не решение какой-то моментной ситуации, а приказы это следствие  Какой-то плановой работой.\n",
            "Это одна из плановых, грубо говоря, составляющих системные работы.\n",
            "И вот здесь вот, когда система построена, ну вот у нас сейчас как построена, и словно еще внедряется там дополнительная приказа, то она может быть заберекратизирована.\n",
            "Ну и в то же время я говорю, есть определенные вещи, по которым люди должны где-то считывать информацию, то есть, да, и, соответственно, принимать ее.\n",
            "Вот у нас она пока не построена.\n",
            "То есть, вот почему как бы вызывают  вот эти вопросы моменты, то есть мы составляем, мы планируем делаем, людям выкатываем, а они говорят, слушайте, а у меня вот такая история, у меня к этому, ну то есть и не задачивается, грубо говоря, чтобы сопоставить свою историю, той истории, которую мы планируем и делаем.\n",
            "Вот, и вот здесь надо, чтобы она работала в едином, грубо говоря, системе вот такое какое-то должно.\n",
            "Может нам просто что-то...\n",
            "Сейчас правила компании доделаем, в тёме надо делывать картинку, там изменения внеслись, в них будет.\n",
            "Может нам что-то организационное сделать, просто какой-то канал организационный, туда разместиться, как-то отдыхает, то что делает.\n",
            "Ну, давайте подумаем.\n",
            "Отложить эту историю, но пока, да, возникает, если оно будет критически накапливаться, там несколько раз вернуть к этому вопросу, то возобнаем либо надо будет поиследовать какую-то историю, чтобы нам ее вытреть.\n",
            "Да, хорошо.\n",
            "нам скорее всего нужен кто-то, кто бы эту работу внедрил к нам.\n",
            "Вот как в свое время флаг внедряли еще кто-то.\n",
            "То есть, кого-то, кто системно может это все дело внедрить.\n",
            "И здесь я бы вернулся к вопросу системного какого-то аналитика, который по процессам у нас идет.\n",
            "Я бы его, может быть, посмотрел бы в живую офис.\n",
            "В свое время в химке хотел человек взять с обещания проводил.\n",
            "Были интересные кандидаты.\n",
            "Сейчас может быть есть смысл, чтобы он работал именно с ребятами, то есть на складе, чтобы там у нас система была и дальше переключался.\n",
            "Я бы тоже это взял вакансию где-нибудь на февраль.\n",
            "по  сотрудника уходит.\n",
            "Я это буду делать, это буду передавать.\n",
            "Это не буду, но, грубо говоря, чеклист.\n",
            "Ключил бы в историю в том, что договор подписываем.\n",
            "Выплаты производятся при условии подписанного чеклиста от руководителя.\n",
            "Если чеклист не подписан, выплаты не производятся.\n",
            "Чтобы это для сотрудника было понятно, зачем он это делает.\n",
            "И, соответственно, нам было понятно, что мы все, что необходимо, забрали.\n",
            "в  Вместе с днём проливы и проливы  нужно принимать решение, работаем мы с ним или нет и как работаем там раз в месяц или раз в неделю куда идем или мы отказываемся от встречи с ними давайте я бы наверное не отказывался совсем бы от встреч вот и сделал бы их раз в месяц ну то есть на подведение каких-то итогов ну то есть в том ключе как мы сейчас там двигаемся  Давайте я попробую с ним обсудить, если смогу сама все согласовать, то смогу, потому что я, по-моему, в прошлый раз решение принимал после общения с вами.\n",
            "Работаем, не работаем и как работаем.\n",
            "Пропишу ему, когда...\n",
            "Когда с Ильей, наверное, сейчас свяжусь.\n",
            "Я когда к нему последний раз заходила, он говорил, что это вопрос собственника и он будет решать самостоятельно с вами, тогда обсудит.\n",
            "Доставку в белую мы откладывали до февраля.\n",
            "Мы бы их дали на программу, где система бизнес-процессов описана, там программа на основе, на примере, одинески, они похожи.\n",
            "То, что из того, что у нас есть, у нас точно такую же структуру процесса компании, бизнес-процессов компании, предлагал Дмитрий Титервак, мы по нему тогда выстраивали работу, когда я пришла на описание,  Мы делали.\n",
            "Титервак в каком виде?\n",
            "Вы мне говорили, да, кидали ссылку bitec.ru.\n",
            "Я их посмотрела несколько их роликов.\n",
            "У них сама программа сделана что-то наподобие одинески.\n",
            "Из того, что у нас уже есть от них, там бизнес процессы выстроены таким образом, то есть построение идет от большого к маленькому процессу, так как мы выстраивали с Титервак с Дмитрием.\n",
            "В целом он предлагает то же самое.\n",
            "Сама по себе параграмма, она не сложная, она наверное больше нудная.\n",
            "Из плюсов я почитила для себя, что у нас получается правильно без процесса нам их нужно в таком русле и достроить, и не менять.\n",
            "Заходить в эту программу наверное смысла нет.\n",
            "Я не сам в своих видео говорю, что это от 100 минимум, в целом от 200-300 сотрудников есть смысл.\n",
            "Когда компания масштабная, большая, есть взаимосвязи.\n",
            "Мне ещё понравилась взаимосвязь паралям.\n",
            "У них расписано как раз таки, когда они выстраивают орбструктуру, они задачи делают, не к человеку привязывают, а к орбструктуру.\n",
            "то что мы с вами начали делать в ноябре и потом оставили это до позже то есть прорисовать оркструктуру именно по ролям да как бы а дальше смотреть кто у нас в этой роли будет исполнителем да и туда уже привязывают человека заходить в эту программу наверное нам нет смысла вот эти моменты почерпнуть  надо и попробовать построить и делать.\n",
            "это другая программа запланировать внедрение этой программы.\n",
            "Опять же обозначить, искать исполнители и договаривать, что с этого времени начнём уже, чтобы с нашей стороны был человек.\n",
            "Мы можем начать, когда будет сотрудник, который будет этим заниматься.\n",
            "Сотрудник перед этим должен работать в компании хотя бы месяц либо два, но понять, что он не уйдёт через месяц, через два, мы опять не начнём кого-то искать.\n",
            "Он должен загрепиться в компании,  и  как там с чабуратором, с водимом, ну то есть нужен конкретный человек, который будет приезжать к нам в офис, то есть скорее всего физически, потому что эффект от этого будет больше, то есть это может дороже стоить, то есть там не полторы тысячи, а там две тысячи или две с половиной в зависимости от специалиста.\n",
            "Вот, здесь я бы опять же искал бы по опыту, по которому, по моему, это на месте либо на инфостарте, либо где-то вот где выкладывают обработки, связанные с маркетплейстами, брал бы уже  человека, который в теме, которым мне надо с нуля там объяснять.\n",
            "Вот один из бы в недрили бы.\n",
            "То есть действовали бы там с планом на год, чтобы год был расписан.\n",
            "Понятно, что могут быть какие-то резкие истории типа вот как у Сергея появилась, как она называется, ну вот это, которую я с выставки притащил программу, с которой он там недренем занимается.\n",
            "Это понятно.\n",
            "Здесь сегодня, да, Индипан, да, здесь сегодня сейчас.\n",
            "Вот, но в целом  Если мы располнируем, мы к этому времени, наверное, и будем.\n",
            "Потому что мне понравилась история с тем, как мы располнировали все-таки по продажам.\n",
            "Я думаю, что эффективность еще и в этом была с наличием товара, а не отсутствуем его.\n",
            "И здесь я бы тоже какие-то планы бы сделал на год.\n",
            "Задача прописала.\n",
            "Когда может корректироваться, но у нас по крайней мере может.\n",
            "будет записано, да, то есть здесь вот так, здесь вот так и нагрузку, но чтобы она распределена была.\n",
            "Ну давайте попробуем.\n",
            "Сложно работаем с планами, у нас все время что-то возникает новое.\n",
            "Да, не, оно будет, ну то есть, ну в любом случае параллельно там, если мы найдем человека, там в этом процессе будет двигаться, то есть мы не будем стопориться.\n",
            "И то, что вчера мы обсуждали в Марину.\n",
            "Я хочу после этого начать выкладывать, чтобы у нас была жизнь в компании.\n",
            "Сейчас как-то мы уходим к тому, что работают все сами по себе.\n",
            "Нет, не, приптих теряется, нет общения.\n",
            "Угу.\n",
            "Сам чек-лист того, как данный вносится менять не стало.\n",
            "Я его предложил Муравьев, потому что ему так удобно.\n",
            "Давайте попробуем по нему поработать.\n",
            "На мой взгляд будет много лишних строчек, но если удобно, пусть останется.\n",
            "Но инициативу не будем обрубать.\n",
            "Ну вот и уже пробуем работать.\n",
            "Поплатим с вами согласовали.\n",
            "Дальше посмотрим по заполнению и чего будет хватать.\n",
            "Не хватать и будем добавлять у выпута.\n",
            "чтобы у нас процесс уже был отработан и мы его тоже в чеклист внесем и админ контроль качества будет контроль выполнения задачи будем его вести  Вы пишете?\n",
            "Нет, я просто нагидаю то, что это сам видел.\n",
            "Ошибки, которые я вижу, Адмитрии не обращает на них внимание.\n",
            "Да, в чекле степи по активности сотрудников нет.\n",
            "То есть то, что нужно смотреть.\n",
            "И не было комментарий по продавцам.\n",
            "Мы делали именно в чеклист, в отделе графу и комментарии, где оператор в свободной форме давал обратную связь по работе продавца.\n",
            "Что он делал, там дело не делал, чем занимался, там активно, неактивно, там было много покупателей, еще что-то.\n",
            "Вот оно было действенно.\n",
            "То есть они действительно прописывали просто пунктами, отмечали там, да, нет, не знаю, а они именно  и от этого уже можно было сделать какой-то вывод идти и соответственно, вот, получим здесь обратную связь Дмитрий смотрит неплохо, он вцепляется каким-то вещам придирчиво, да, здесь вопрос может быть нам попробовать инну из этого процесса убрать в просмотре  работать именно с Дмитрием, потому что мне кажется, что там влияние друг на друга есть и на какие-то свои желания, как называется, свое видение процесса диктует Дмитрию.\n",
            "И как бы не обрубает инициативу.\n",
            "Я могу ошибаться, но вот кажется, что нужно здесь посмотреть, что надо.\n",
            "Можно попробовать смотреть, сейчас у нас в принципе точек нету есть склад, но склад важный.\n",
            "Какие-то подробности я когда начал отсматривать, увидел, что есть недоработки с учетом определенных задач.\n",
            "Здесь можно не углубляться в работу склада, то есть подробно.\n",
            "Может быть я там сейчас где-то перегибаю, и это не должно быть.\n",
            "То есть основная задача это, как я говорил, три показателя.\n",
            "Они складываются.\n",
            "уровню показателя, которые складываются из тех, которые, ну в принципе, у нас есть.\n",
            "Но мы не можем выпускать мелкие показатели.\n",
            "То есть если мелкие показатели выпускать, то тоже это самое.\n",
            "На этапе этого самого начала можно потеряться.\n",
            "Я здесь согласна, но только вопрос это должны не выделать, наверное, руководитель склада.\n",
            "Он не видит в этом необходимости, у него все в порядке.\n",
            "Вот в этом и проблемы, что картины не сходятся.\n",
            "Вот как его нужно заставить это видеть получается.\n",
            "Пока подпинуть, а в целом смотрим еще ребята.\n",
            "потому что я понимаю, что у него есть картины мира, склад и все, а я ему рассказываю о том, что склад — это фундаментальная история, которая должна самой собой работать и мы должны двигаться дальше.\n",
            "Дальше это улучшение работы по складу, оптимизация работы, доработка динесовцев, еще что-то.\n",
            "Встреча с Лебедевой не будет.\n",
            "День завтра встреча должна быть, вероника тоже завтра должна быть.\n",
            "Я не знаю.\n",
            "Есть еще вопросы?\n",
            "Нет, вопросов нет, тогда до завтра.\n",
            "До встречи.\n",
            "Да, ты там связи.\n",
            "Thank you.\n",
            "-----------------------------------------------------------------\n",
            "Ключевые моменты встречи\n",
            "-----------------------------------------------------------------\n",
            "**Резюме встречи:**\n",
            "Встреча началась с представления команд и разъяснения ролей\n",
            "Были выявлены и рассмотрены потенциальные проблемы и вопросы\n",
            "\n",
            "**Темы обсуждения:**\n",
            "- Планирование и реализация проекта\n",
            "- Маркетинговая и рекламная стратегия\n",
            "\n",
            "**Действия:**\n",
            "- [Я буду работать над созданием более четкого чек-листа для процесса продукции\n",
            "- чтобы он был более структурирован и включал все необходимые пункты\n",
            "- чтобы мы могли его легко изменять и вносить изменения.]\n",
            "- Команда наметила задачи\n",
            "- которые необходимо выполнить. Обсуждались задачи планирования и организации. Членам команды были поручены исследовательские мероприятия.\n",
            "\n",
            "**Проблемы:**\n",
            "- Никто.\n",
            "- Были выявлены потенциальные проблемы и трудности.\n",
            "\n",
            "**Решения:**\n",
            "- Встреча была очень короткой и не очень структурированной.\n",
            "- Никаких решений принято не было.\n",
            "- Никаких конкретных вопросов не было задано.\n",
            "- Никаких конкретных решений принято не было.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Путь к видеофайлу с записью встречи\n",
        "video_path = \"./examples/meeting_video.mp4\"\n",
        "\n",
        "# Извлекаем аудиодорожку из видео\n",
        "audio_path = extract_audio(video_path)\n",
        "\n",
        "# Разбиваем аудиофайл на фрагменты\n",
        "chunk_paths = split_audio(audio_path)\n",
        "\n",
        "# Проводим транскрипцию аудиофрагментов с помощью модели распознавания речи\n",
        "full_transcription = get_transcription(chunk_paths)\n",
        "\n",
        "# Анализируем стенограмму встречи и извлекаем ключевые моменты\n",
        "meeting_info = get_meeting_info(full_transcription)\n",
        "\n",
        "# Очищаем JSON-данные встречи от дубликатов и вложенных элементов\n",
        "cleaned_meeting_info = clean_meeting_summary(meeting_info)\n",
        "\n",
        "# Преобразуем структурированные данные в удобочитаемый текстовый формат\n",
        "formatted_text = format_meeting_info(cleaned_meeting_info)\n",
        "\n",
        "# Выводим результат\n",
        "print(\"\\n-----------------------------------------------------------------\")\n",
        "print(\"Транскрипция встречи\")\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "print(format_transcription(full_transcription))  # Форматируем стенограмму, чтобы каждое предложение было на новой строке\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "print(\"Ключевые моменты встречи\")\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "print(formatted_text)  # Выводим структурированное резюме встречи"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}